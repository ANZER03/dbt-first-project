# Datalake

Local analytics sandbox that loads seed data into DuckDB using dbt. The `etl` directory contains the dbt project, while DuckDB artifacts live at the repository root for quick inspection and ad-hoc analysis.

## Repository layout
- `etl/` – dbt project with models, seeds, macros, and target artifacts
- `my_ducklake.duckdb` – DuckDB database generated by dbt builds
- `logs/` – top-level run logs emitted during orchestration
- `.venv/` – optional Python virtual environment for dbt tooling (not tracked in Git)

## Prerequisites
- Python 3.10+
- [dbt-duckdb](https://docs.getdbt.com/docs/core/connect-data-platform/duckdb-setup) and supporting dbt packages
- DuckDB CLI (optional) for exploring `my_ducklake.duckdb`

## Getting started
```bash
python -m venv .venv
# Windows PowerShell
. .venv/Scripts/Activate.ps1
# or bash
source .venv/bin/activate

pip install dbt-core dbt-duckdb
```

The dbt profile is stored in `etl/.dbt/profiles.yml` and points at the DuckDB database in this repository. Adjust it if you need a different location.

## Typical workflow
```bash
cd etl

dbt deps      # install dbt packages (none by default)
dbt seed      # load CSVs from etl/seeds into DuckDB
dbt run       # build staging and mart models
dbt test      # execute data quality tests
```

Artifacts from runs are written to `etl/target/` and log output goes to `etl/logs/`. The generated DuckDB file can be queried directly via the DuckDB CLI or any compatible tool.

## Useful references
- [dbt documentation](https://docs.getdbt.com/)
- [DuckDB documentation](https://duckdb.org/docs/)
